{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import os\n",
    "from stable_baselines3 import PPO  #PPO"
   ],
   "id": "204a5222b7cf3fdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from carla_park_env import CarlaParkingEnv",
   "id": "db3829f90b0cbaef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('setting folders for logs and models')\n",
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ],
   "id": "3c89ca0edc7974f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('connecting to env..')\n",
    "env = CarlaParkingEnv()"
   ],
   "id": "e255fe5c0ccf398f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env.reset()\n",
    "print('Env has been reset as part of launch')"
   ],
   "id": "97f19e2a24c4ce29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = (\n",
    "    PPO(\n",
    "        'MlpPolicy',\n",
    "        env,\n",
    "        verbose=2,\n",
    "        learning_rate=1e-5,\n",
    "        clip_range=0.5,  #clip_range en PPO para permitir más exploración:\n",
    "        tensorboard_log=logdir,\n",
    "        device='cuda'\n",
    "    )\n",
    ")"
   ],
   "id": "83b153ec206533e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "f2c4b07f6d9e6496"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ProgressCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int, log_dir: str):\n",
    "        super(ProgressCallback, self).__init__()\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.rewards = []  # Para almacenar las recompensas por episodio\n",
    "        self.episode_rewards = []  # Para almacenar la recompensa promedio por episodio\n",
    "        self.episodes = 0  # Contador de episodios\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Usar la recompensa registrada en 'rewards' directamente\n",
    "            mean_reward = np.mean(self.locals['rewards'])  # Promedio de recompensas recientes\n",
    "\n",
    "            self.rewards.append(mean_reward)\n",
    "            self.episodes += 1  # Aumenta el número de episodios\n",
    "            print(f\"Step: {self.n_calls}, Mean Reward (últimos {self.check_freq} pasos): {mean_reward}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Al finalizar el entrenamiento, graficar recompensas vs episodios\n",
    "        self.plot_rewards()\n",
    "\n",
    "    def plot_rewards(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(self.episodes), self.rewards, label=\"Recompensa Promedio\")\n",
    "        plt.xlabel(\"Episodios\")\n",
    "        plt.ylabel(\"Recompensa Promedio\")\n",
    "        plt.title(\"Recompensa Promedio por Episodio\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ],
   "id": "d4957dcd1fed6478"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "total_timesteps = 10000 # how many timesteps you want to train for\n",
    "progress_callback = ProgressCallback(check_freq=10, log_dir=logdir)\n",
    "model.learn(total_timesteps=total_timesteps, callback=progress_callback)\n",
    "model.save(\"ppo_carla_parking\")"
   ],
   "id": "a4e95643957147a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "TIMESTEPS = 5  # how long is each training iteration - individual steps\n",
    "iters = 0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_episodes = 10\n",
    "rewards_per_episode = []"
   ],
   "id": "b47bdbed16067c91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# while iters < 5:  # how many training iterations you want\n",
    "#     iters += 1\n",
    "#     print('Iteration ', iters, ' is to commence...')\n",
    "#     model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "#     print('Iteration ', iters, ' has been trained')\n",
    "#     model.save(f\"{models_dir}/{TIMESTEPS * iters}\")"
   ],
   "id": "e897cc804c7e206d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Evaluar el modelo después de cada iteración\n",
    "# for _ in range(num_episodes):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     episode_reward = 0\n",
    "#     while not done:\n",
    "#         action, _states = model.predict(obs)\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         episode_reward += reward\n",
    "#     rewards_per_episode.append(episode_reward)\n",
    "#\n",
    "#\n"
   ],
   "id": "5c32867e181159b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # --- Generar gráfico de recompensas ---\n",
    "# plt.plot(rewards_per_episode)\n",
    "# plt.title('Recompensas por Episodio')\n",
    "# plt.xlabel('Episodio')\n",
    "# plt.ylabel('Recompensa Total')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ],
   "id": "38dd5ab265711e37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from stable_baselines3.common.evaluation import evaluate_policy",
   "id": "ded43887e7ee8aa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ller el modelo entrenado\n",
    "model = PPO.load(\"1729585704\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "id": "b57fa9bbf0ba62f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "spectator = env.world.get_spectator()\n",
    "\n",
    "transform = spectator.get_transform()\n",
    "location = transform.location\n",
    "rotation = transform.rotation\n",
    "\n",
    "print(location, rotation)"
   ],
   "id": "6820c1f22ed4063c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"GPU disponible:\", tf.config.list_physical_devices())"
   ],
   "id": "2395eb1a47dac960"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# v es el valor que se quiere penalizar\n",
    "# k controla la inclinación de la curva (qué tan rápido crece la penalización)\n",
    "# m controla la penalización maxima\n",
    "# p controla la magnitud de la penalización\n",
    "\n",
    "mm=60\n",
    "\n",
    "def punish_logistic(v, k=0.1, m=mm, p=1):\n",
    "    logistic = 2 / (1 + np.exp(-k * (v - m)))\n",
    "    return logistic * p\n",
    "\n",
    "# Generar valores de tiempo entre 0 y 60\n",
    "tiempos = np.linspace(0, mm, 100)\n",
    "penalizaciones = punish_logistic(tiempos)\n",
    "\n",
    "# Graficar\n",
    "plt.plot(tiempos, penalizaciones)\n",
    "plt.title(\"Penalización respecto al tiempo\")\n",
    "plt.xlabel(\"Tiempo (segundos)\")\n",
    "plt.ylabel(\"Penalización\")\n",
    "plt.show()"
   ],
   "id": "676bbb52cd06ae1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tensorboard --logdir='E:\\UADY\\CARLA\\CARLA_Latest\\WindowsNoEditor\\PythonAPI\\examples\\Ruben\\Parking training\\logs'",
   "id": "2a67b68bf70856a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
